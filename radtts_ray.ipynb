{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee34a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import ray\n",
    "# from ray.data.datasource import FastFileMetadataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5acc542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from uberduck_ml_dev.vocoders.hifigan import AttrDict, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ed12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Generator(torch.nn.Module):\n",
    "#     __constants__ = ['lrelu_slope', 'num_kernels', 'num_upsamples', 'p_blur']\n",
    "#     def __init__(self, h):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.num_kernels = len(h.resblock_kernel_sizes)\n",
    "#         self.num_upsamples = len(h.upsample_rates)\n",
    "#         self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
    "#         self.p_blur = h.gaussian_blur['p_blurring']\n",
    "#         self.gaussian_blur_fn = None\n",
    "#         if self.p_blur > 0.0:\n",
    "#             self.gaussian_blur_fn = GaussianBlurAugmentation(h.gaussian_blur['kernel_size'], h.gaussian_blur['sigmas'], self.p_blur)\n",
    "#         else:\n",
    "#             self.gaussian_blur_fn = nn.Identity()\n",
    "#         self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "#         resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
    "\n",
    "#         self.ups = nn.ModuleList()\n",
    "#         for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
    "#             self.ups.append(weight_norm(\n",
    "#                 ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
    "#                                 k, u, padding=(k-u)//2)))\n",
    "\n",
    "#         self.resblocks = nn.ModuleList()\n",
    "#         for i in range(len(self.ups)):\n",
    "#             resblock_list = nn.ModuleList()\n",
    "#             ch = h.upsample_initial_channel//(2**(i+1))\n",
    "#             for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
    "#                 resblock_list.append(resblock(h, ch, k, d))\n",
    "#             self.resblocks.append(resblock_list)\n",
    "\n",
    "#         self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
    "#         self.ups.apply(init_weights)\n",
    "#         self.conv_post.apply(init_weights)\n",
    "\n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         new_state_dict = {}\n",
    "#         for k, v in state_dict.items():\n",
    "#             new_k = k\n",
    "#             if 'resblocks' in k:\n",
    "#                 parts = k.split(\".\")\n",
    "#                 # only do this is the checkpoint type is older\n",
    "#                 if len(parts) == 5:\n",
    "#                     layer = int(parts[1])\n",
    "#                     new_layer = f\"{layer//3}.{layer%3}\"\n",
    "#                     new_k = f\"resblocks.{new_layer}.{'.'.join(parts[2:])}\"\n",
    "#             new_state_dict[new_k] = v\n",
    "#         super().load_state_dict(new_state_dict)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.p_blur > 0.0:\n",
    "#             x = self.gaussian_blur_fn(x)\n",
    "#         x = self.conv_pre(x)\n",
    "#         for upsample_layer, resblock_group in zip(self.ups, self.resblocks):\n",
    "#             x = F.leaky_relu(x, self.lrelu_slope)\n",
    "#             x = upsample_layer(x)\n",
    "#             xs = torch.zeros(x.shape, dtype=x.dtype, device=x.device)\n",
    "#             for resblock in resblock_group:\n",
    "#                 xs += resblock(x)\n",
    "#             x = xs / self.num_kernels\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.conv_post(x)\n",
    "#         x = torch.tanh(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def remove_weight_norm(self):\n",
    "#         print('Removing weight norm...')\n",
    "#         for l in self.ups:\n",
    "#             remove_weight_norm(l)\n",
    "#         for group in self.resblocks:\n",
    "#             for block in group:\n",
    "#                 block.remove_weight_norm()\n",
    "#         remove_weight_norm(self.conv_pre)\n",
    "#         remove_weight_norm(self.conv_post)\n",
    "\n",
    "# class ResBlock1(torch.nn.Module):\n",
    "#     __constants__ = ['lrelu_slope']\n",
    "#     def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "#         super(ResBlock1, self).__init__()\n",
    "#         self.h = h\n",
    "#         self.lrelu_slope = LRELU_SLOPE\n",
    "#         self.convs1 = nn.ModuleList([\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "#                                padding=get_padding(kernel_size, dilation[0]))),\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "#                                padding=get_padding(kernel_size, dilation[1]))),\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "#                                padding=get_padding(kernel_size, dilation[2])))\n",
    "#         ])\n",
    "#         self.convs1.apply(init_weights)\n",
    "\n",
    "#         self.convs2 = nn.ModuleList([\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "#                                padding=get_padding(kernel_size, 1))),\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "#                                padding=get_padding(kernel_size, 1))),\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "#                                padding=get_padding(kernel_size, 1)))\n",
    "#         ])\n",
    "#         self.convs2.apply(init_weights)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for c1, c2 in zip(self.convs1, self.convs2):\n",
    "#             xt = F.leaky_relu(x, self.lrelu_slope)\n",
    "#             xt = c1(xt)\n",
    "#             xt = F.leaky_relu(xt, self.lrelu_slope)\n",
    "#             xt = c2(xt)\n",
    "#             x = xt + x\n",
    "#         return x\n",
    "\n",
    "#     def remove_weight_norm(self):\n",
    "#         for l in self.convs1:\n",
    "#             remove_weight_norm(l)\n",
    "#         for l in self.convs2:\n",
    "#             remove_weight_norm(l)\n",
    "\n",
    "\n",
    "# class ResBlock2(torch.nn.Module):\n",
    "#     __constants__ = ['lrelu_slope']\n",
    "#     def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
    "#         super(ResBlock2, self).__init__()\n",
    "#         self.h = h\n",
    "#         self.convs = nn.ModuleList([\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "#                                padding=get_padding(kernel_size, dilation[0]))),\n",
    "#             weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "#                                padding=get_padding(kernel_size, dilation[1])))\n",
    "#         ])\n",
    "#         self.convs.apply(init_weights)\n",
    "#         self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for c in self.convs:\n",
    "#             xt = F.leaky_relu(x, self.lrelu_slope)\n",
    "#             xt = c(xt)\n",
    "#             x = xt + x\n",
    "#         return x\n",
    "\n",
    "#     def remove_weight_norm(self):\n",
    "#         for l in self.convs:\n",
    "#             remove_weight_norm(l)\n",
    "\n",
    "            \n",
    "# def get_padding(kernel_size, dilation=1):\n",
    "#     return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "# def init_weights(m, mean=0.0, std=0.01):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find(\"Conv\") != -1:\n",
    "#         m.weight.data.normal_(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cd9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
    "import torch.nn as nn\n",
    "LRELU_SLOPE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef14e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIFI_GAN_CONFIG_URL = \"https://uberduck-models-us-west-2.s3.us-west-2.amazonaws.com/hifigan_22khz_config.json\"\n",
    "HIFI_GAN_GENERATOR_URL = \"https://uberduck-models-us-west-2.s3.us-west-2.amazonaws.com/hifigan_libritts100360_generator0p5.pt\"\n",
    "from io import BytesIO\n",
    "response = requests.get(HIFI_GAN_CONFIG_URL)\n",
    "hifigan_config = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c0ba74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# state_dict_g = torch.load(vocoder_path, map_location='cpu')['generator']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# load hifigan\u001b[39;00m\n\u001b[1;32m     11\u001b[0m vocoder \u001b[38;5;241m=\u001b[39m Generator(h)\n\u001b[0;32m---> 12\u001b[0m vocoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mloaded\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# denoiser = Denoiser(vocoder)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# if to_cuda:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m vocoder\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded' is not defined"
     ]
    }
   ],
   "source": [
    "h = AttrDict(hifigan_config)\n",
    "if 'gaussian_blur' in hifigan_config:\n",
    "    hifigan_config['gaussian_blur']['p_blurring'] = 0.0\n",
    "else:\n",
    "    hifigan_config['gaussian_blur'] = {'p_blurring': 0.0}\n",
    "    h['gaussian_blur'] = {'p_blurring': 0.0}\n",
    "\n",
    "# state_dict_g = torch.load(vocoder_path, map_location='cpu')['generator']\n",
    "\n",
    "# load hifigan\n",
    "vocoder = Generator(h)\n",
    "vocoder.load_state_dict(loaded['generator'])\n",
    "# denoiser = Denoiser(vocoder)\n",
    "# if to_cuda:\n",
    "vocoder.cuda()\n",
    "# denoiser.cuda()\n",
    "vocoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f837d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(AttrDict(hifigan_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff38c214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "response = requests.get(HIFI_GAN_GENERATOR_URL, stream=True)\n",
    "bio = BytesIO(response.content)\n",
    "loaded = torch.load(bio)\n",
    "model.load_state_dict(loaded['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5622cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting model config...\n",
      "Loading pretrained model...\n",
      "Got pretrained model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  )\n",
       "  (resblocks): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uberduck_ml_dev.exec.train_radtts_with_ray import get_vocoder\n",
    "\n",
    "get_vocoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4745416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lj_df = pd.read_csv(\n",
    "#     \"https://uberduck-datasets-dirty.s3.us-west-2.amazonaws.com/lj_for_upload/metadata_formatted_100.txt\",\n",
    "#     sep=\"|\",\n",
    "#     header=None,\n",
    "#     quoting=3,\n",
    "#     names=[\"path\", \"transcript\", \"speaker_id\"], # pitch path is implicit - this should be changed\n",
    "# )\n",
    "\n",
    "# paths = lj_df.path.tolist()\n",
    "# transcripts = lj_df.transcript.tolist()\n",
    "# speaker_ids = lj_df.speaker_id.tolist()\n",
    "\n",
    "# parallelism_length = 400\n",
    "# audio_ds = ray.data.read_binary_files(\n",
    "#     paths,\n",
    "#     parallelism=parallelism_length,\n",
    "#     meta_provider=FastFileMetadataProvider(),\n",
    "#     ray_remote_args={\"num_cpus\": 0.2},\n",
    "# )\n",
    "# audio_ds = audio_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "\n",
    "# paths = ray.data.from_items(paths, parallelism=parallelism_length)\n",
    "# paths_ds = paths.map_batches(lambda x: x, batch_format=\"pyarrow\", batch_size=None)\n",
    "\n",
    "# transcripts = ray.data.from_items(transcripts, parallelism=parallelism_length)\n",
    "# transcripts_ds = transcripts.map_batches(lambda x: x, batch_format=\"pyarrow\", batch_size=None)\n",
    "\n",
    "# speaker_ids_ds = ray.data.from_items(speaker_ids, parallelism=parallelism_length)\n",
    "# speaker_ids_ds = speaker_ids_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "# output_dataset = (\n",
    "#     transcripts_ds.zip(audio_ds)\n",
    "#     .zip(paths_ds)\n",
    "#     .zip(speaker_ids_ds)\n",
    "# )\n",
    "# output_dataset = output_dataset.map_batches(\n",
    "#     lambda table: table.rename(\n",
    "#         columns={\n",
    "#             \"value_1\": \"transcript\",\n",
    "#             \"value_2\": \"audio_bytes\",\n",
    "#             \"value_3\": \"path\",\n",
    "#             \"value_4\": \"speaker_id\"\n",
    "#         }\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c879d041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datum = ray_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad4fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ray/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "2023-03-08 13:49:12,366\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.38.167:6379...\n",
      "2023-03-08 13:49:12,372\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_ha6my3r3kl4mn9jdylcqnfutq1/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "RuntimeEnvSetupError",
     "evalue": "Failed to set up runtime environment.\nFailed to upload package /tmp/ray_latest_runtime_env.zip to the Ray cluster: Package size (893.72MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:64\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     working_dir_uri \u001b[38;5;241m=\u001b[39m \u001b[43mget_uri_for_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcludes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcludes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:  \u001b[38;5;66;03m# working_dir is not a directory\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:455\u001b[0m, in \u001b[0;36mget_uri_for_directory\u001b[0;34m(directory, excludes)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be an existing directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m hash_val \u001b[38;5;241m=\u001b[39m _hash_directory(directory, directory, _get_excludes(directory, excludes))\n",
      "\u001b[0;31mValueError\u001b[0m: directory /tmp/ray_latest_runtime_env.zip must be an existing directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:75\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mupload_package_to_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:480\u001b[0m, in \u001b[0;36mupload_package_to_gcs\u001b[0;34m(pkg_uri, pkg_bytes)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;241m==\u001b[39m Protocol\u001b[38;5;241m.\u001b[39mGCS:\n\u001b[0;32m--> 480\u001b[0m     \u001b[43m_store_package_in_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m Protocol\u001b[38;5;241m.\u001b[39mremote_protocols():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:321\u001b[0m, in \u001b[0;36m_store_package_in_gcs\u001b[0;34m(pkg_uri, data, logger)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m GCS_STORAGE_MAX_SIZE:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeds the maximum size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_mib_string(GCS_STORAGE_MAX_SIZE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can exclude large \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles using the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcludes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m option to the runtime_env or provide \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma remote URI of a zip file using protocols such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and so on, refer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushing file package \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to Ray cluster...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Package size (893.72MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeEnvSetupError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m             state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_data_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_data_dict(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_data_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28msuper\u001b[39m(TorchCheckpoint, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m__setstate__(state)\n\u001b[0;32m---> 13\u001b[0m ray_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_ray_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m train_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_group_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_group_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m train_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdur_model_config\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdur_model_config\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/default/uberduck_ml_dev/exec/train_radtts_with_ray.py:521\u001b[0m, in \u001b[0;36mget_ray_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    518\u001b[0m speaker_ids \u001b[38;5;241m=\u001b[39m lj_df\u001b[38;5;241m.\u001b[39mspeaker_id\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    520\u001b[0m parallelism_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m--> 521\u001b[0m audio_ds \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_binary_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# meta_provider=FastFileMetadataProvider(),\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_cpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m audio_ds \u001b[38;5;241m=\u001b[39m audio_ds\u001b[38;5;241m.\u001b[39mmap_batches(\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x, batch_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m paths \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfrom_items(paths, parallelism\u001b[38;5;241m=\u001b[39mparallelism_length)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/read_api.py:1146\u001b[0m, in \u001b[0;36mread_binary_files\u001b[0;34m(paths, include_paths, filesystem, parallelism, ray_remote_args, arrow_open_stream_args, meta_provider, partition_filter, partitioning)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_binary_files\u001b[39m(\n\u001b[1;32m   1102\u001b[0m     paths: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     partitioning: Partitioning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1112\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset[Union[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m], \u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a dataset from binary files of arbitrary contents.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m        Dataset holding Arrow records read from the specified paths.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBinaryDatasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopen_stream_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_open_stream_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/read_api.py:282\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    279\u001b[0m     ray_remote_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduling_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPREAD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m force_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m cur_pg \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_placement_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m pa_ds \u001b[38;5;241m=\u001b[39m _lazy_import_pyarrow_dataset()\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa_ds:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/util/placement_group.py:316\u001b[0m, in \u001b[0;36mget_current_placement_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_placement_group\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[PlacementGroup]:\n\u001b[1;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the current placement group which a task or actor is using.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    It returns None if there's no current placement group for the worker.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            created with any placement group.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclient_mode_should_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauto_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# Client mode is only a driver.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     worker \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mglobal_worker\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:124\u001b[0m, in \u001b[0;36mclient_mode_should_convert\u001b[0;34m(auto_init)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m         os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_ENABLE_AUTO_CONNECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# `is_client_mode_enabled_by_default` is used for testing with\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# `RAY_CLIENT_MODE=1`. This flag means all tests run with client mode.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    129\u001b[0m     is_client_mode_enabled \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default\n\u001b[1;32m    130\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m _get_client_hook_status_on_thread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:1559\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1557\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1559\u001b[0m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_object_store_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_driver_object_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entrypoint_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mand\u001b[39;00m job_config\u001b[38;5;241m.\u001b[39mcode_search_path:\n\u001b[1;32m   1572\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mset_load_code_from_local(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:2035\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   2031\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m job_config\u001b[38;5;241m.\u001b[39mruntime_env \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   2032\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m upload_py_modules_if_needed(\n\u001b[1;32m   2033\u001b[0m     runtime_env, scratch_dir, logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m   2034\u001b[0m )\n\u001b[0;32m-> 2035\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m \u001b[43mupload_working_dir_if_needed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruntime_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;66;03m# Remove excludes, it isn't relevant after the upload step.\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m runtime_env\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcludes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:77\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m     upload_package_to_gcs(pkg_uri, package_path\u001b[38;5;241m.\u001b[39mread_bytes())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RuntimeEnvSetupError(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload package \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the Ray cluster: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     80\u001b[0m runtime_env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pkg_uri\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m runtime_env\n",
      "\u001b[0;31mRuntimeEnvSetupError\u001b[0m: Failed to set up runtime environment.\nFailed to upload package /tmp/ray_latest_runtime_env.zip to the Ray cluster: Package size (893.72MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference."
     ]
    }
   ],
   "source": [
    "from uberduck_ml_dev.exec.train_radtts_with_ray import get_ray_dataset, train_func, train_config, model_config\n",
    "from ray.train.torch import TorchTrainer, TorchCheckpoint, TorchTrainer\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "from ray.tune import SyncConfig\n",
    "\n",
    "class TorchCheckpointFixed(TorchCheckpoint):\n",
    "    def __setstate__(self, state: dict):\n",
    "        if \"_data_dict\" in state and state[\"_data_dict\"]:\n",
    "            state = state.copy()\n",
    "            state[\"_data_dict\"] = self._decode_data_dict(state[\"_data_dict\"])\n",
    "        super(TorchCheckpoint, self).__setstate__(state)\n",
    "\n",
    "ray_dataset = get_ray_dataset()\n",
    "train_config['n_group_size'] = model_config['n_group_size']\n",
    "train_config['dur_model_config'] = model_config['dur_model_config']\n",
    "train_config['f0_model_config'] = model_config['f0_model_config']\n",
    "train_config['energy_model_config'] = model_config['energy_model_config']\n",
    "train_config['v_model_config']=model_config['v_model_config']\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2, use_gpu=True, resources_per_worker=dict(CPU=4, GPU=1)\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        sync_config=SyncConfig(upload_dir=\"s3://uberduck-anyscale-data/checkpoints\")\n",
    "    ),\n",
    "    datasets={\"train\": ray_dataset},\n",
    ")\n",
    "\n",
    "# result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5b910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b1ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4e1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstart_checkpoint_path = \"/home/ray/default/radtts++ljs-dap.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69e5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.exec.train_radtts_with_ray import model_config, ray_df_to_batch_radtts, get_ray_dataset\n",
    "from uberduck_ml_dev.models.radtts import RADTTS\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe876fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying spectral norm to text encoder LSTM\n",
      "Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/default/uberduck_ml_dev/models/common.py:1516: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "  W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "LU, pivots = torch.lu(A, compute_pivots)\n",
      "should be replaced with\n",
      "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "and\n",
      "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "should be replaced with\n",
      "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    }
   ],
   "source": [
    "model = RADTTS(\n",
    "    **model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9e02c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 13:39:55,924\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.2.79:6379...\n",
      "2023-03-08 13:39:55,935\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_ha6my3r3kl4mn9jdylcqnfutq1/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "RuntimeEnvSetupError",
     "evalue": "Failed to set up runtime environment.\nFailed to upload package /tmp/ray_latest_runtime_env.zip to the Ray cluster: Package size (893.69MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:64\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     working_dir_uri \u001b[38;5;241m=\u001b[39m \u001b[43mget_uri_for_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcludes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcludes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:  \u001b[38;5;66;03m# working_dir is not a directory\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:455\u001b[0m, in \u001b[0;36mget_uri_for_directory\u001b[0;34m(directory, excludes)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be an existing directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m hash_val \u001b[38;5;241m=\u001b[39m _hash_directory(directory, directory, _get_excludes(directory, excludes))\n",
      "\u001b[0;31mValueError\u001b[0m: directory /tmp/ray_latest_runtime_env.zip must be an existing directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:75\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mupload_package_to_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:480\u001b[0m, in \u001b[0;36mupload_package_to_gcs\u001b[0;34m(pkg_uri, pkg_bytes)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;241m==\u001b[39m Protocol\u001b[38;5;241m.\u001b[39mGCS:\n\u001b[0;32m--> 480\u001b[0m     \u001b[43m_store_package_in_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m Protocol\u001b[38;5;241m.\u001b[39mremote_protocols():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py:321\u001b[0m, in \u001b[0;36m_store_package_in_gcs\u001b[0;34m(pkg_uri, data, logger)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m GCS_STORAGE_MAX_SIZE:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeds the maximum size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_mib_string(GCS_STORAGE_MAX_SIZE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can exclude large \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles using the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcludes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m option to the runtime_env or provide \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma remote URI of a zip file using protocols such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and so on, refer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushing file package \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to Ray cluster...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Package size (893.69MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeEnvSetupError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ray_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_ray_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/uberduck_ml_dev/exec/train_radtts_with_ray.py:521\u001b[0m, in \u001b[0;36mget_ray_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    518\u001b[0m speaker_ids \u001b[38;5;241m=\u001b[39m lj_df\u001b[38;5;241m.\u001b[39mspeaker_id\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    520\u001b[0m parallelism_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m--> 521\u001b[0m audio_ds \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_binary_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# meta_provider=FastFileMetadataProvider(),\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_cpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m audio_ds \u001b[38;5;241m=\u001b[39m audio_ds\u001b[38;5;241m.\u001b[39mmap_batches(\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x, batch_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m paths \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfrom_items(paths, parallelism\u001b[38;5;241m=\u001b[39mparallelism_length)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/read_api.py:1146\u001b[0m, in \u001b[0;36mread_binary_files\u001b[0;34m(paths, include_paths, filesystem, parallelism, ray_remote_args, arrow_open_stream_args, meta_provider, partition_filter, partitioning)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_binary_files\u001b[39m(\n\u001b[1;32m   1102\u001b[0m     paths: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     partitioning: Partitioning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1112\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset[Union[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m], \u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a dataset from binary files of arbitrary contents.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m        Dataset holding Arrow records read from the specified paths.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBinaryDatasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopen_stream_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_open_stream_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/read_api.py:282\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    279\u001b[0m     ray_remote_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduling_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPREAD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m force_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m cur_pg \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_placement_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m pa_ds \u001b[38;5;241m=\u001b[39m _lazy_import_pyarrow_dataset()\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa_ds:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/util/placement_group.py:316\u001b[0m, in \u001b[0;36mget_current_placement_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_placement_group\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[PlacementGroup]:\n\u001b[1;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the current placement group which a task or actor is using.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    It returns None if there's no current placement group for the worker.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            created with any placement group.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclient_mode_should_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauto_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# Client mode is only a driver.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     worker \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mglobal_worker\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:124\u001b[0m, in \u001b[0;36mclient_mode_should_convert\u001b[0;34m(auto_init)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m         os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_ENABLE_AUTO_CONNECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# `is_client_mode_enabled_by_default` is used for testing with\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# `RAY_CLIENT_MODE=1`. This flag means all tests run with client mode.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    129\u001b[0m     is_client_mode_enabled \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default\n\u001b[1;32m    130\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m _get_client_hook_status_on_thread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:1559\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1557\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1559\u001b[0m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_object_store_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_driver_object_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entrypoint_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mand\u001b[39;00m job_config\u001b[38;5;241m.\u001b[39mcode_search_path:\n\u001b[1;32m   1572\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mset_load_code_from_local(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:2035\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   2031\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m job_config\u001b[38;5;241m.\u001b[39mruntime_env \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   2032\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m upload_py_modules_if_needed(\n\u001b[1;32m   2033\u001b[0m     runtime_env, scratch_dir, logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m   2034\u001b[0m )\n\u001b[0;32m-> 2035\u001b[0m runtime_env \u001b[38;5;241m=\u001b[39m \u001b[43mupload_working_dir_if_needed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruntime_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;66;03m# Remove excludes, it isn't relevant after the upload step.\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m runtime_env\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcludes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py:77\u001b[0m, in \u001b[0;36mupload_working_dir_if_needed\u001b[0;34m(runtime_env, scratch_dir, logger, upload_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m     upload_package_to_gcs(pkg_uri, package_path\u001b[38;5;241m.\u001b[39mread_bytes())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RuntimeEnvSetupError(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload package \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the Ray cluster: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     80\u001b[0m runtime_env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pkg_uri\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m runtime_env\n",
      "\u001b[0;31mRuntimeEnvSetupError\u001b[0m: Failed to set up runtime environment.\nFailed to upload package /tmp/ray_latest_runtime_env.zip to the Ray cluster: Package size (893.69MiB) exceeds the maximum size of 500.00MiB. You can exclude large files using the 'excludes' option to the runtime_env or provide a remote URI of a zip file using protocols such as 's3://', 'https://' and so on, refer to https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference."
     ]
    }
   ],
   "source": [
    "ray_dataset = get_ray_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shard = session.get_dataset_shard(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, ray_batch_df = next(enumerate(\n",
    "    dataset_shard.iter_batches(batch_size=batch_size)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        batch_dict = ray_df_to_batch_radtts(batch)\n",
    "        mel = to_gpu(batch_dict['mel'])\n",
    "        speaker_ids = to_gpu(batch_dict['speaker_ids'])\n",
    "        attn_prior = to_gpu(batch_dict['attn_prior'])\n",
    "        f0 = to_gpu(batch_dict['f0'])\n",
    "        voiced_mask = to_gpu(batch_dict['voiced_mask'])\n",
    "        p_voiced = to_gpu(batch_dict['p_voiced'])\n",
    "        text = to_gpu(batch_dict['text'])\n",
    "        in_lens = to_gpu(batch_dict['input_lengths'])\n",
    "        out_lens = to_gpu(batch_dict['output_lengths'])\n",
    "        energy_avg = to_gpu(batch_dict['energy_avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6e1390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m----> 2\u001b[0m             \u001b[43mmel\u001b[49m, speaker_ids, text, in_lens, out_lens,\n\u001b[1;32m      3\u001b[0m             binarize_attention\u001b[38;5;241m=\u001b[39mbinarize, attn_prior\u001b[38;5;241m=\u001b[39mattn_prior,\n\u001b[1;32m      4\u001b[0m             f0\u001b[38;5;241m=\u001b[39mf0, energy_avg\u001b[38;5;241m=\u001b[39menergy_avg,\n\u001b[1;32m      5\u001b[0m             voiced_mask\u001b[38;5;241m=\u001b[39mvoiced_mask, p_voiced\u001b[38;5;241m=\u001b[39mp_voiced)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mel' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = model(\n",
    "            mel, speaker_ids, text, in_lens, out_lens,\n",
    "            binarize_attention=binarize, attn_prior=attn_prior,\n",
    "            f0=f0, energy_avg=energy_avg,\n",
    "            voiced_mask=voiced_mask, p_voiced=p_voiced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31442c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e41a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a32ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f95c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b7829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fe723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bee533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937da0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15805565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cebf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b2f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a8ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f352bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.integrations.wandb import  setup_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56d685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function setup_wandb in module ray.air.integrations.wandb:\n",
      "\n",
      "setup_wandb(config: Optional[Dict] = None, api_key: Optional[str] = None, api_key_file: Optional[str] = None, rank_zero_only: bool = True, **kwargs) -> Union[wandb.sdk.wandb_run.Run, wandb.sdk.lib.disabled.RunDisabled]\n",
      "    Set up a Weights & Biases session.\n",
      "    \n",
      "    This function can be used to initialize a Weights & Biases session in a\n",
      "    (distributed) training or tuning run.\n",
      "    \n",
      "    By default, the run ID is the trial ID, the run name is the trial name, and\n",
      "    the run group is the experiment name. These settings can be overwritten by\n",
      "    passing the respective arguments as ``kwargs``, which will be passed to\n",
      "    ``wandb.init()``.\n",
      "    \n",
      "    In distributed training with Ray Train, only the zero-rank worker will initialize\n",
      "    wandb. All other workers will return a disabled run object, so that logging is not\n",
      "    duplicated in a distributed run. This can be disabled by passing\n",
      "    ``rank_zero_only=False``, which will then initialize wandb in every training\n",
      "    worker.\n",
      "    \n",
      "    The ``config`` argument will be passed to Weights and Biases and will be logged\n",
      "    as the run configuration.\n",
      "    \n",
      "    If no API key or key file are passed, wandb will try to authenticate\n",
      "    using locally stored credentials, created for instance by running ``wandb login``.\n",
      "    \n",
      "    Keyword arguments passed to ``setup_wandb()`` will be passed to\n",
      "    ``wandb.init()`` and take precedence over any potential default settings.\n",
      "    \n",
      "    Args:\n",
      "        config: Configuration dict to be logged to Weights and Biases. Can contain\n",
      "            arguments for ``wandb.init()`` as well as authentication information.\n",
      "        api_key: API key to use for authentication with Weights and Biases.\n",
      "        api_key_file: File pointing to API key for with Weights and Biases.\n",
      "        rank_zero_only: If True, will return an initialized session only for the\n",
      "            rank 0 worker in distributed training. If False, will initialize a\n",
      "            session for all workers.\n",
      "        kwargs: Passed to ``wandb.init()``.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "        .. code-block: python\n",
      "    \n",
      "            from ray.air.integrations.wandb import wandb_setup\n",
      "    \n",
      "            def training_loop(config):\n",
      "                wandb = wandb_setup(config)\n",
      "                # ...\n",
      "                wandb.log({\"loss\": 0.123})\n",
      "    \n",
      "    **PublicAPI (alpha):** This API is in alpha and may change before becoming stable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(setup_wandb) #entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593b8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 16:35:52,269\tWARNING session.py:100 -- In neither tune session nor train session!\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `get_trial_id` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n",
      "2023-03-07 16:35:52,271\tWARNING session.py:100 -- In neither tune session nor train session!\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `get_trial_name` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n",
      "2023-03-07 16:35:52,272\tWARNING session.py:100 -- In neither tune session nor train session!\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `get_experiment_name` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n",
      "wandb: Currently logged in as: uberduck. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ray/default/wandb/run-20230307_163553-e0id39cs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uberduck/radtts-ray/runs/e0id39cs' target=\"_blank\">fresh-brook-47</a></strong> to <a href='https://wandb.ai/uberduck/radtts-ray' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uberduck/radtts-ray' target=\"_blank\">https://wandb.ai/uberduck/radtts-ray</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uberduck/radtts-ray/runs/e0id39cs' target=\"_blank\">https://wandb.ai/uberduck/radtts-ray/runs/e0id39cs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/uberduck/radtts-ray/runs/e0id39cs?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0524702da0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = train_config\n",
    "setup_wandb(config, project=\"radtts-ray\", rank_zero_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852fb000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA AVAILABLE:  True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RADTTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m kl_loss_start_iter \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl_loss_start_iter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m binarization_start_iter \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinarization_start_iter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRADTTS\u001b[49m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mprepare_model(model, parallel_strategy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(find_unused_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RADTTS' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA AVAILABLE: \", torch.cuda.is_available())\n",
    "epochs = config[\"epochs\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "steps_per_sample = config[\"steps_per_sample\"]\n",
    "# gin_channels = config[\"gin_channels\"]\n",
    "sigma = config['sigma']\n",
    "kl_loss_start_iter = config['kl_loss_start_iter']\n",
    "binarization_start_iter = config['binarization_start_iter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa001b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In neither tune session nor train session!\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `get_world_size` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `1`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.air import session\n",
    "session.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f9f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_world_size in module ray.air.session:\n",
      "\n",
      "get_world_size() -> int\n",
      "    Get the current world size (i.e. total number of workers) for this run.\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "        import time\n",
      "        from ray.air import session\n",
      "        from ray.air.config import ScalingConfig\n",
      "    \n",
      "        def train_loop_per_worker(config):\n",
      "            assert session.get_world_size() == 4\n",
      "    \n",
      "        train_dataset = ray.data.from_items(\n",
      "            [{\"x\": x, \"y\": x + 1} for x in range(32)])\n",
      "        trainer = TensorflowTrainer(train_loop_per_worker,\n",
      "            scaling_config=ScalingConfig(num_workers=1),\n",
      "            datasets={\"train\": train_dataset})\n",
      "        trainer.fit()\n",
      "    \n",
      "    **PublicAPI (beta):** This API is in beta and may change before becoming stable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(session.get_world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa7c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying spectral norm to text encoder LSTM\n",
      "Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ExponentialLR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     14\u001b[0m     lr \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     weight_decay \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mExponentialLR\u001b[49m(\n\u001b[1;32m     18\u001b[0m     optim,\n\u001b[1;32m     19\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m dataset_shard \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_dataset_shard(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ExponentialLR' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +53m35s) Adding 1 node(s) of type worker-node-type-0.\n",
      "(autoscaler +53m35s) Adding 1 node(s) of type worker-node-type-1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(raylet, ip=10.0.3.123) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.3.123)   aiogrpc.init_grpc_aio()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +54m36s) Resized to 100 CPUs, 1 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(raylet, ip=10.0.13.139) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.13.139)   aiogrpc.init_grpc_aio()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +54m50s) Resized to 104 CPUs, 2 GPUs.\n",
      "(autoscaler +55m16s) Adding 1 node(s) of type worker-node-type-0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(raylet, ip=10.0.20.186) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.20.186)   aiogrpc.init_grpc_aio()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +56m16s) Resized to 120 CPUs, 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(raylet) [2023-03-07 17:31:47,223 E 369 369] (raylet) node_manager.cc:3040: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c896a1f2d49a841de85d9f4ade62b71676469b02e6133224652550d8, IP: 10.0.37.211) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.37.211`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2023-03-07 17:32:41,086 E 1169933 1176438] core_worker.cc:569: :info_message: Attempting to recover 300 lost objects by resubmitting their tasks. To disable object reconstruction, set @ray.remote(max_retries=0).\n",
      "*** SIGTERM received at time=1678239163 on cpu 3 ***\n",
      "PC: @     0x7f064729346e  (unknown)  epoll_wait\n",
      "    @     0x7f06474d4420  (unknown)  (unknown)\n",
      "[2023-03-07 17:32:43,764 E 1169933 1169947] logging.cc:361: *** SIGTERM received at time=1678239163 on cpu 3 ***\n",
      "[2023-03-07 17:32:43,764 E 1169933 1169947] logging.cc:361: PC: @     0x7f064729346e  (unknown)  epoll_wait\n",
      "[2023-03-07 17:32:43,764 E 1169933 1169947] logging.cc:361:     @     0x7f06474d4420  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "from uberduck_ml_dev.models.radtts import RADTTS\n",
    "\n",
    "model = RADTTS(\n",
    "    **model_config,\n",
    ")\n",
    "import ray.train as train\n",
    "# model = train.torch.prepare_model(model, parallel_strategy_kwargs = dict(find_unused_parameters=True))\n",
    "checkpoint_dict = None\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = config[\"learning_rate\"],\n",
    "    weight_decay = config[\"weight_decay\"]\n",
    ")\n",
    "scheduler = ExponentialLR(\n",
    "    optim,\n",
    "    config[\"weight_decay\"],\n",
    "    last_epoch=-1,\n",
    ")\n",
    "dataset_shard = session.get_dataset_shard(\"train\")\n",
    "global_step = 0\n",
    "scaler = GradScaler()\n",
    "\n",
    "criterion = RADTTSLoss(\n",
    "    sigma,\n",
    "    config['n_group_size'],\n",
    "    config['dur_model_config'],\n",
    "    config['f0_model_config'],\n",
    "    config['energy_model_config'],\n",
    "    vpred_model_config=config['v_model_config'],\n",
    "    loss_weights=config['loss_weights']\n",
    ")\n",
    "attention_kl_loss = AttentionBinarizationLoss()\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# discriminator = MultiPeriodDiscriminator(MODEL_CONFIG[\"use_spectral_norm\"])\n",
    "# discriminator = train.torch.prepare_model(discriminator)\n",
    "\n",
    "checkpoint_dict = _load_checkpoint_dict()\n",
    "if checkpoint_dict is None:\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "else:\n",
    "    global_step = checkpoint_dict[\"global_step\"]\n",
    "    start_epoch = checkpoint_dict[\"epoch\"]\n",
    "    if session.get_world_size() > 1:\n",
    "        model_sd = checkpoint_dict[\"model\"]\n",
    "        # NOTE(zach): Add audio embedding state dict if it is not present.\n",
    "        # NOTE(zach): Pass strict=False due to different nuber of gin_channels\n",
    "        model.load_state_dict(model_sd, strict=False)\n",
    "    else:\n",
    "        model_sd = _fix_state_dict(checkpoint_dict[\"model\"])\n",
    "        # NOTE(zach): Pass strict=False due to different nuber of gin_channels\n",
    "        model.load_state_dict(model_sd, strict=False)\n",
    "    del checkpoint_dict\n",
    "\n",
    "# NOTE (Sam): replace with RAdam\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = config[\"learning_rate\"],\n",
    "    weight_decay = config[\"weight_decay\"]\n",
    ")\n",
    "scheduler = ExponentialLR(\n",
    "    optim,\n",
    "    config[\"weight_decay\"],\n",
    "    last_epoch=-1,\n",
    ")\n",
    "dataset_shard = session.get_dataset_shard(\"train\")\n",
    "global_step = 0\n",
    "scaler = GradScaler()\n",
    "\n",
    "criterion = RADTTSLoss(\n",
    "    sigma,\n",
    "    config['n_group_size'],\n",
    "    config['dur_model_config'],\n",
    "    config['f0_model_config'],\n",
    "    config['energy_model_config'],\n",
    "    vpred_model_config=config['v_model_config'],\n",
    "    loss_weights=config['loss_weights']\n",
    ")\n",
    "attention_kl_loss = AttentionBinarizationLoss()\n",
    "iteration = 0\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    for batch_idx, ray_batch_df in enumerate(\n",
    "        dataset_shard.iter_batches(batch_size=batch_size)\n",
    "    ):\n",
    "        torch.cuda.empty_cache()\n",
    "        _train_step(\n",
    "            ray_batch_df,\n",
    "            model,\n",
    "            optim,\n",
    "            global_step,\n",
    "            steps_per_sample,\n",
    "            scaler,\n",
    "            scheduler,\n",
    "            criterion,\n",
    "            attention_kl_loss,\n",
    "            iteration,\n",
    "            kl_loss_start_iter,\n",
    "            binarization_start_iter,\n",
    "        )\n",
    "        global_step += 1\n",
    "    if session.get_world_rank() == 0:\n",
    "        # TODO(zach): Also save wandb artifact here.\n",
    "        checkpoint = Checkpoint.from_dict(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                global_step=global_step,\n",
    "                model=model.state_dict(),\n",
    "            )\n",
    "        )\n",
    "        session.report({}, checkpoint=checkpoint)\n",
    "        artifact = wandb.Artifact(\n",
    "            f\"artifact_epoch{epoch}_step{global_step}\", \"model\"\n",
    "        )\n",
    "        with tempfile.TemporaryDirectory() as tempdirname:\n",
    "            checkpoint.to_directory(tempdirname)\n",
    "            artifact.add_dir(tempdirname)\n",
    "            wandb.log_artifact(artifact)\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431fad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 16:19:08,608\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.37.211:6379...\n",
      "2023-03-07 16:19:08,633\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_ha6my3r3kl4mn9jdylcqnfutq1/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-03-07 16:19:08,653\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9.zip' (6.85MiB) to Ray cluster...\n",
      "2023-03-07 16:19:08,744\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9.zip'.\n",
      "(_get_read_tasks pid=5467, ip=10.0.46.245) 2023-03-07 16:19:10,287\tWARNING file_meta_provider.py:162 -- Expanding 100 path(s). This may be a HIGH LATENCY operation on some cloud storage services. If the specified paths all point to files and never directories, try rerunning this read with `meta_provider=FastFileMetadataProvider()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-07 16:23:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:44.92        </td></tr>\n",
       "<tr><td>Memory:      </td><td>7.1/15.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 9.0/184 CPUs, 2.0/6 GPUs, 0.0/484.53 GiB heap, 0.0/208.25 GiB objects (0.0/6.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_d9af1_00000</td><td>RUNNING </td><td>10.0.46.245:5550</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 16:19:11,683\tWARNING trial_runner.py:1333 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (202 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) 2023-03-07 16:19:23,587\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) 2023-03-07 16:19:24,250\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> AllToAllOperator[zip] -> AllToAllOperator[zip] -> AllToAllOperator[zip] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> AllToAllOperator[randomize_block_order]\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) 2023-03-07 16:19:27,668\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) 2023-03-07 16:19:33,962\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) 2023-03-07 16:19:34,065\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) /home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/dataset_iterator.py:64: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset/DatasetPipeline as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245)   warnings.warn(\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) [nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) [nltk_data]     /home/ray/nltk_data...\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) [nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: Currently logged in as: uberduck. Use `wandb login --relogin` to force relogin\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: Currently logged in as: uberduck. Use `wandb login --relogin` to force relogin\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: wandb version 0.13.11 is available!  To upgrade, please run:\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb:  $ pip install wandb --upgrade\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: Tracking run with wandb version 0.13.10\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: Run data is saved locally in /home/ray/ray_results/TorchTrainer_2023-03-07_16-19-11/TorchTrainer_d9af1_00000_0_2023-03-07_16-19-13/rank_1/wandb/run-20230307_161948-d9af1_00000\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: Run `wandb offline` to turn off syncing.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: Syncing run TorchTrainer_d9af1_00000\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: ⭐️ View project at https://wandb.ai/uberduck/radtts-ray\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) wandb: 🚀 View run at https://wandb.ai/uberduck/radtts-ray/runs/d9af1_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=417, ip=10.0.17.31) CUDA AVAILABLE:  True\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Applying spectral norm to text encoder LSTM\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=417, ip=10.0.17.31) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9/uberduck_ml_dev/models/common.py:1516: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Q, R = torch.qr(A, some)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) should be replaced with\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) LU, pivots = torch.lu(A, compute_pivots)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) should be replaced with\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) and\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) should be replaced with\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: wandb version 0.13.11 is available!  To upgrade, please run:\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb:  $ pip install wandb --upgrade\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: Tracking run with wandb version 0.13.10\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: Run data is saved locally in /home/ray/ray_results/TorchTrainer_2023-03-07_16-19-11/TorchTrainer_d9af1_00000_0_2023-03-07_16-19-13/rank_0/wandb/run-20230307_161948-d9af1_00000\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: Run `wandb offline` to turn off syncing.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: Syncing run TorchTrainer_d9af1_00000\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: ⭐️ View project at https://wandb.ai/uberduck/radtts-ray\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) wandb: 🚀 View run at https://wandb.ai/uberduck/radtts-ray/runs/d9af1_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) CUDA AVAILABLE:  True\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) Applying spectral norm to text encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9/uberduck_ml_dev/models/common.py:1516: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) Q, R = torch.qr(A, some)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) should be replaced with\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) LU, pivots = torch.lu(A, compute_pivots)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) should be replaced with\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) and\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) should be replaced with\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=417, ip=10.0.17.31) 2023-03-07 16:20:06,289\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) 2023-03-07 16:20:06,824\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) 2023-03-07 16:20:08,136\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) 2023-03-07 16:20:09,592\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9/uberduck_ml_dev/exec/train_radtts_with_ray.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   audio = torch.FloatTensor(wav_data)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_ee2356972b78dbfda1fe20807a72a9e9/uberduck_ml_dev/exec/train_radtts_with_ray.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   audio = torch.FloatTensor(wav_data)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) grad.sizes() = [1, 512], strides() = [1, 1]\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) grad.sizes() = [1, 512], strides() = [1, 1]\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) Loss: 66.75837707519531\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Loss: 66.7209243774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=417, ip=10.0.17.31) Loss: 5.840334415435791\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) Loss: 5.75288200378418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=417, ip=10.0.17.31) Loss: 5.8561625480651855\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178) Loss: 5.837704658508301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=235, ip=10.0.45.178)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=235, ip=10.0.45.178) Loss: 5.733274459838867\n",
      "(RayTrainWorker pid=417, ip=10.0.17.31) Loss: 5.9500956535339355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 16:23:53,988\tWARNING tune.py:150 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-03-07 16:23:56,613\tERROR tune.py:821 -- Trials did not complete: [TorchTrainer_d9af1_00000]\n",
      "2023-03-07 16:23:56,614\tINFO tune.py:825 -- Total run time: 285.09 seconds (283.31 seconds for the tuning loop).\n",
      "2023-03-07 16:23:56,614\tWARNING tune.py:831 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "(TorchTrainer pid=5550, ip=10.0.46.245) 2023-03-07 16:23:56,616\tINFO utils.py:57 -- Worker 1 has failed.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ec2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff97a9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "613fd88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\n",
    "        \"TORCH_DISTRIBUTED_DEBUG\"\n",
    "    ] = \"DETAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dd57cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-07 15:42:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:32:54.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.6/15.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/2 GPUs, 0.0/61.09 GiB heap, 0.0/26.77 GiB objects (0.0/2.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_209d8_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ray/ray_results/TorchTrainer_2023-03-07_15-09-35/TorchTrainer_209d8_00000_0_2023-03-07_15-09-37/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_209d8_00000</td><td>ERROR   </td><td>10.0.48.48:3738</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) 2023-03-07 15:09:46,491\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:09:46,610\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> AllToAllOperator[zip] -> AllToAllOperator[zip] -> AllToAllOperator[zip] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> AllToAllOperator[randomize_block_order]\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:09:49,880\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:09:55,809\tWARNING plan.py:528 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:09:55,809\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:09:55,955\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +26s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(autoscaler +26s) Adding 5 node(s) of type worker-node-type-0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(TorchTrainer pid=3738, ip=10.0.48.48) /home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/dataset_iterator.py:64: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset/DatasetPipeline as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48)   warnings.warn(\n",
      "(RayTrainWorker pid=1124519) wandb: Currently logged in as: uberduck. Use `wandb login --relogin` to force relogin\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: Currently logged in as: uberduck. Use `wandb login --relogin` to force relogin\n",
      "(RayTrainWorker pid=1124519) wandb: wandb version 0.13.11 is available!  To upgrade, please run:\n",
      "(RayTrainWorker pid=1124519) wandb:  $ pip install wandb --upgrade\n",
      "(RayTrainWorker pid=1124519) wandb: Tracking run with wandb version 0.13.10\n",
      "(RayTrainWorker pid=1124519) wandb: Run data is saved locally in /home/ray/ray_results/TorchTrainer_2023-03-07_15-09-35/TorchTrainer_209d8_00000_0_2023-03-07_15-09-37/rank_1/wandb/run-20230307_151001-209d8_00000\n",
      "(RayTrainWorker pid=1124519) wandb: Run `wandb offline` to turn off syncing.\n",
      "(RayTrainWorker pid=1124519) wandb: Syncing run TorchTrainer_209d8_00000\n",
      "(RayTrainWorker pid=1124519) wandb: ⭐️ View project at https://wandb.ai/uberduck/radtts-ray\n",
      "(RayTrainWorker pid=1124519) wandb: 🚀 View run at https://wandb.ai/uberduck/radtts-ray/runs/209d8_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) CUDA AVAILABLE:  True\n",
      "(RayTrainWorker pid=1124519) Applying spectral norm to text encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_eccc6979c79037b7d167507f1405cae4/uberduck_ml_dev/models/common.py:1516: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=1124519) The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "(RayTrainWorker pid=1124519) Q, R = torch.qr(A, some)\n",
      "(RayTrainWorker pid=1124519) should be replaced with\n",
      "(RayTrainWorker pid=1124519) Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "(RayTrainWorker pid=1124519)   W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=1124519) LU, pivots = torch.lu(A, compute_pivots)\n",
      "(RayTrainWorker pid=1124519) should be replaced with\n",
      "(RayTrainWorker pid=1124519) LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "(RayTrainWorker pid=1124519) and\n",
      "(RayTrainWorker pid=1124519) LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "(RayTrainWorker pid=1124519) should be replaced with\n",
      "(RayTrainWorker pid=1124519) LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "(RayTrainWorker pid=1124519)   return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) Applying spectral norm to context encoder LSTM\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) CUDA AVAILABLE:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: wandb version 0.13.11 is available!  To upgrade, please run:\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb:  $ pip install wandb --upgrade\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: Tracking run with wandb version 0.13.10\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: Run data is saved locally in /home/ray/ray_results/TorchTrainer_2023-03-07_15-09-35/TorchTrainer_209d8_00000_0_2023-03-07_15-09-37/rank_0/wandb/run-20230307_151002-209d8_00000\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: Run `wandb offline` to turn off syncing.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: Syncing run TorchTrainer_209d8_00000\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: ⭐️ View project at https://wandb.ai/uberduck/radtts-ray\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) wandb: 🚀 View run at https://wandb.ai/uberduck/radtts-ray/runs/209d8_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) Applying spectral norm to text encoder LSTM\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_eccc6979c79037b7d167507f1405cae4/uberduck_ml_dev/models/common.py:1516: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) Q, R = torch.qr(A, some)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) should be replaced with\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) LU, pivots = torch.lu(A, compute_pivots)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) should be replaced with\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) and\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) should be replaced with\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n",
      "(RayTrainWorker pid=1124519) 2023-03-07 15:10:19,948\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) 2023-03-07 15:10:20,219\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=1124519) 2023-03-07 15:10:21,843\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) 2023-03-07 15:10:22,005\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=1124519) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_eccc6979c79037b7d167507f1405cae4/uberduck_ml_dev/exec/train_radtts_with_ray.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "(RayTrainWorker pid=1124519)   audio = torch.FloatTensor(wav_data)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /tmp/ray/session_2023-03-06_08-59-42_925397_141/runtime_resources/working_dir_files/_ray_pkg_eccc6979c79037b7d167507f1405cae4/uberduck_ml_dev/exec/train_radtts_with_ray.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   audio = torch.FloatTensor(wav_data)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(raylet, ip=10.0.14.59) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.14.59)   aiogrpc.init_grpc_aio()\n",
      "(raylet, ip=10.0.12.220) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.12.220)   aiogrpc.init_grpc_aio()\n",
      "(raylet, ip=10.0.57.173) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.57.173)   aiogrpc.init_grpc_aio()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=1124519)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +1m22s) Resized to 136 CPUs, 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(raylet, ip=10.0.21.18) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.21.18)   aiogrpc.init_grpc_aio()\n",
      "(raylet, ip=10.0.55.4) /home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "(raylet, ip=10.0.55.4)   aiogrpc.init_grpc_aio()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) Loss: 66.58922576904297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) grad.sizes() = [1, 512], strides() = [1, 1]\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "(RayTrainWorker pid=1124519) grad.sizes() = [1, 512], strides() = [1, 1]\n",
      "(RayTrainWorker pid=1124519) bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
      "(RayTrainWorker pid=1124519)   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "(RayTrainWorker pid=1124519)   warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) Loss: 66.71593475341797\n",
      "(autoscaler +1m26s) Resized to 184 CPUs, 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=1124519)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) Loss: 66.75285339355469\n",
      "(RayTrainWorker pid=1124519) Loss: 66.76640319824219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=1124519)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) Loss: 66.75389099121094\n",
      "(RayTrainWorker pid=1124519) Loss: 66.77154541015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=303, ip=10.0.5.7)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=1124519)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=303, ip=10.0.5.7) Loss: 66.65056610107422\n",
      "(RayTrainWorker pid=1124519) Loss: 66.45819854736328\n",
      "(autoscaler +6m24s) Removing 2 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m28s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m34s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m38s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m44s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m48s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m54s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +6m59s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m4s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m8s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m14s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m18s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m24s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m29s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m34s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m39s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m44s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m49s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m54s) Removing 5 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7m59s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m4s) Resized to 120 CPUs, 2 GPUs.\n",
      "(autoscaler +8m4s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m9s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m14s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m19s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m25s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +8m35s) Resized to 104 CPUs, 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1124519) [E ProcessGroupNCCL.cpp:821] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=123, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800693 milliseconds before timing out.\n",
      "(RayTrainWorker pid=1124519) [E ProcessGroupNCCL.cpp:456] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "(RayTrainWorker pid=1124519) [E ProcessGroupNCCL.cpp:461] To avoid data inconsistency, we are taking the entire process down.\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,362 E 1124519 1124595] logging.cc:97: Unhandled exception: St13runtime_error. what(): [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=123, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800693 milliseconds before timing out.\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "(RayTrainWorker pid=1124519)   result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,404 E 1124519 1124595] logging.cc:104: Stack trace: \n",
      "(RayTrainWorker pid=1124519)  /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so(+0xd5105a) [0x7faa7d9ff05a] ray::operator<<()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so(+0xd53818) [0x7faa7da01818] ray::TerminateHandler()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/bin/../lib/libstdc++.so.6(+0xb135a) [0x7faa7cb4135a] __cxxabiv1::__terminate()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/bin/../lib/libstdc++.so.6(+0xb13c5) [0x7faa7cb413c5]\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/bin/../lib/libstdc++.so.6(+0xb134f) [0x7faa7cb4134f]\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cuda_cpp.so(_ZN4c10d16ProcessGroupNCCL8WorkNCCL15handleNCCLGuardENS_17ErrorHandlingModeE+0x278) [0x7fa8b398f8b8] c10d::ProcessGroupNCCL::WorkNCCL::handleNCCLGuard()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cuda_cpp.so(_ZN4c10d16ProcessGroupNCCL15workCleanupLoopEv+0x19f) [0x7fa8b39938cf] c10d::ProcessGroupNCCL::workCleanupLoop()\n",
      "(RayTrainWorker pid=1124519) /home/ray/anaconda3/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7faa7cb6bbf4] execute_native_thread_routine\n",
      "(RayTrainWorker pid=1124519) /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609) [0x7faa7e7e0609] start_thread\n",
      "(RayTrainWorker pid=1124519) /usr/lib/x86_64-linux-gnu/libc.so.6(clone+0x43) [0x7faa7e5ab133] __clone\n",
      "(RayTrainWorker pid=1124519) \n",
      "(RayTrainWorker pid=1124519) *** SIGABRT received at time=1678232537 on cpu 2 ***\n",
      "(RayTrainWorker pid=1124519) PC: @     0x7faa7e4cf00b  (unknown)  raise\n",
      "(RayTrainWorker pid=1124519)     @     0x7faa7e7ec420       4016  (unknown)\n",
      "(RayTrainWorker pid=1124519)     @     0x7faa7cb4135a  (unknown)  __cxxabiv1::__terminate()\n",
      "(RayTrainWorker pid=1124519)     @     0x7faa7cb41070  (unknown)  (unknown)\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,406 E 1124519 1124595] logging.cc:361: *** SIGABRT received at time=1678232537 on cpu 2 ***\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,406 E 1124519 1124595] logging.cc:361: PC: @     0x7faa7e4cf00b  (unknown)  raise\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,406 E 1124519 1124595] logging.cc:361:     @     0x7faa7e7ec420       4016  (unknown)\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,406 E 1124519 1124595] logging.cc:361:     @     0x7faa7cb4135a  (unknown)  __cxxabiv1::__terminate()\n",
      "(RayTrainWorker pid=1124519) [2023-03-07 15:42:17,406 E 1124519 1124595] logging.cc:361:     @     0x7faa7cb41070  (unknown)  (unknown)\n",
      "(RayTrainWorker pid=1124519) Fatal Python error: Aborted\n",
      "(RayTrainWorker pid=1124519) \n",
      "(RayTrainWorker pid=1124519) \n",
      "(RayTrainWorker pid=1124519) Extension modules: msgpack._cmsgpack, setproctitle, google.protobuf.pyext._message, psutil._psutil_linux, psutil._psutil_posix, grpc._cython.cygrpc, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, yaml._yaml, ray._raylet, _cffi_backend, charset_normalizer.md, lz4._version, lz4.frame._frame, zstandard.backend_c, pyarrow.lib, pyarrow._hdfsio, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pydantic.typing, pydantic.errors, pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config, pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks, pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields, pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses, pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools, pydantic, pyarrow._json, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack\n",
      "(RayTrainWorker pid=1124519) , scipy.linalg.cython_blas\n",
      "(RayTrainWorker pid=1124519) , scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, scipy.signal._sigtools, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, numpy.linalg.lapack_lite, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.signal._sosfilt, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.signal._spectral, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.runtime._nrt_python, numba.np.ufunc._internal, xxhash._xxhash, scipy.fftpack.convolve, sklearn.__check_build._check_build, sklearn.utils.murmurhash, sklearn.utils._isfinite, sklearn.utils._openmp_helpers, sklearn.decomposition._cdnmf_fast, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.utils._typedefs, sklearn.utils._readonly_array_wrapper, sklearn.metrics._dist_metrics, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.utils.arrayfuncs, sklearn.linear_model._cd_fast, sklearn._loss._loss, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.linear_model._sag_fast, sklearn.svm._libsvm, sklearn.svm._liblinear, sklearn.svm._libsvm_sparse, sklearn.decomposition._online_lda_fast, sklearn.neighbors._partition_nodes, sklearn.neighbors._ball_tree, sklearn.neighbors._kd_tree, sklearn._isotonic, sklearn.manifold._utils, sklearn.tree._utils, sklearn.tree._tree, sklearn.tree._splitter, sklearn.tree._criterion, sklearn.neighbors._quad_tree, sklearn.manifold._barnes_hut_tsne, sklearn.cluster._k_means_common, sklearn.cluster._k_means_minibatch, sklearn.cluster._k_means_lloyd, sklearn.cluster._k_means_elkan, sklearn.utils._fast_dict, sklearn.cluster._hierarchical_fast, sklearn.cluster._dbscan_inner, sklearn.feature_extraction._hashing_fast, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, regex._regex, pycrfsuite._pycrfsuite, sklearn.datasets._svmlight_format_fast, lxml._elementpath, lxml.etree, numba.experimental.jitclass._box (total: 288)\n",
      "2023-03-07 15:42:17,840\tWARNING worker.py:1870 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffd92daefe0821b2bbe7d2ce9537000000 Worker ID: 41ed1214279d48f74be588edf9b2dad3b8af30a3d1b82a89f8748441 Node ID: c896a1f2d49a841de85d9f4ade62b71676469b02e6133224652550d8 Worker IP address: 10.0.37.211 Worker port: 10459 Worker PID: 1124519 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "(TorchTrainer pid=3738, ip=10.0.48.48) 2023-03-07 15:42:17,840\tINFO utils.py:57 -- Worker 1 has failed.2023-03-07 15:42:17,865\tERROR trial_runner.py:705 -- Trial TorchTrainer_209d8_00000: Error happened when processing _ExecutorEventType.TRAINING_RESULT.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1311, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py\", line 2384, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError: \u001b[36mray::_Inner.train()\u001b[39m (pid=3738, ip=10.0.48.48, repr=TorchTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 360, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
      "    return self._trainable_func(\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 737, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 647, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 433, in training_loop\n",
      "    self._report(training_iterator)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 380, in _report\n",
      "    for results in training_iterator:\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 134, in __next__\n",
      "    next_results = self._run_with_error_handling(self._fetch_next_result)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 97, in _run_with_error_handling\n",
      "    return func()\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 168, in _fetch_next_result\n",
      "    results = self._backend_executor.get_next_results()\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 444, in get_next_results\n",
      "    results = self.get_with_failure_handling(futures)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 533, in get_with_failure_handling\n",
      "    self._increment_failures()\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 595, in _increment_failures\n",
      "    raise failure\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: RayTrainWorker\n",
      "\tactor_id: d92daefe0821b2bbe7d2ce9537000000\n",
      "\tpid: 1124519\n",
      "\tnamespace: 3cec299d-7eb2-44ca-a17e-0e4e4c288f63\n",
      "\tip: 10.0.37.211\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>date               </th><th>hostname     </th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_209d8_00000</td><td>2023-03-07_15-09-43</td><td>ip-10-0-48-48</td><td>10.0.48.48</td><td style=\"text-align: right;\"> 3738</td><td style=\"text-align: right;\"> 1678230583</td><td>209d8_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:42:30,245\tERROR tune.py:821 -- Trials did not complete: [TorchTrainer_209d8_00000]\n",
      "2023-03-07 15:42:30,246\tINFO tune.py:825 -- Total run time: 1974.69 seconds (1962.16 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray::_Inner.train()\u001b[39m (pid=3738, ip=10.0.48.48, repr=TorchTrainer)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 360, in train\n    raise skipped from exception_cause(skipped)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n    return self._trainable_func(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 737, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n    output = fn()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 647, in train_func\n    trainer.training_loop()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 433, in training_loop\n    self._report(training_iterator)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 380, in _report\n    for results in training_iterator:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 134, in __next__\n    next_results = self._run_with_error_handling(self._fetch_next_result)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 97, in _run_with_error_handling\n    return func()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 168, in _fetch_next_result\n    results = self._backend_executor.get_next_results()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 444, in get_next_results\n    results = self.get_with_failure_handling(futures)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 533, in get_with_failure_handling\n    self._increment_failures()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 595, in _increment_failures\n    raise failure\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n\tclass_name: RayTrainWorker\n\tactor_id: d92daefe0821b2bbe7d2ce9537000000\n\tpid: 1124519\n\tnamespace: 3cec299d-7eb2-44ca-a17e-0e4e4c288f63\n\tip: 10.0.37.211\nThe actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py:579\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m     result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m--> 579\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TuneError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRayTaskError\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=3738, ip=10.0.48.48, repr=TorchTrainer)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 360, in train\n    raise skipped from exception_cause(skipped)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n    return self._trainable_func(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 737, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n    output = fn()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 647, in train_func\n    trainer.training_loop()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 433, in training_loop\n    self._report(training_iterator)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 380, in _report\n    for results in training_iterator:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 134, in __next__\n    next_results = self._run_with_error_handling(self._fetch_next_result)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 97, in _run_with_error_handling\n    return func()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/trainer.py\", line 168, in _fetch_next_result\n    results = self._backend_executor.get_next_results()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 444, in get_next_results\n    results = self.get_with_failure_handling(futures)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 533, in get_with_failure_handling\n    self._increment_failures()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 595, in _increment_failures\n    raise failure\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n\tclass_name: RayTrainWorker\n\tactor_id: d92daefe0821b2bbe7d2ce9537000000\n\tpid: 1124519\n\tnamespace: 3cec299d-7eb2-44ca-a17e-0e4e4c288f63\n\tip: 10.0.37.211\nThe actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors."
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2aaa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa_mel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhtk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'slaney'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;34m@\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhtk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"slaney\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Create a Mel filter-bank.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This produces a linear transformation matrix to project\u001b[0m\n",
       "\u001b[0;34m    FFT bins onto Mel-frequency bins.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    sr        : number > 0 [scalar]\u001b[0m\n",
       "\u001b[0;34m        sampling rate of the incoming signal\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_fft     : int > 0 [scalar]\u001b[0m\n",
       "\u001b[0;34m        number of FFT components\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_mels    : int > 0 [scalar]\u001b[0m\n",
       "\u001b[0;34m        number of Mel bands to generate\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    fmin      : float >= 0 [scalar]\u001b[0m\n",
       "\u001b[0;34m        lowest frequency (in Hz)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    fmax      : float >= 0 [scalar]\u001b[0m\n",
       "\u001b[0;34m        highest frequency (in Hz).\u001b[0m\n",
       "\u001b[0;34m        If `None`, use ``fmax = sr / 2.0``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    htk       : bool [scalar]\u001b[0m\n",
       "\u001b[0;34m        use HTK formula instead of Slaney\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    norm : {None, 'slaney', or number} [scalar]\u001b[0m\n",
       "\u001b[0;34m        If 'slaney', divide the triangular mel weights by the width of the mel band\u001b[0m\n",
       "\u001b[0;34m        (area normalization).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        If numeric, use `librosa.util.normalize` to normalize each filter by to unit l_p norm.\u001b[0m\n",
       "\u001b[0;34m        See `librosa.util.normalize` for a full description of supported norm values\u001b[0m\n",
       "\u001b[0;34m        (including `+-np.inf`).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Otherwise, leave all the triangles aiming for a peak value of 1.0\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    dtype : np.dtype\u001b[0m\n",
       "\u001b[0;34m        The data type of the output basis.\u001b[0m\n",
       "\u001b[0;34m        By default, uses 32-bit (single-precision) floating point.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns\u001b[0m\n",
       "\u001b[0;34m    -------\u001b[0m\n",
       "\u001b[0;34m    M         : np.ndarray [shape=(n_mels, 1 + n_fft/2)]\u001b[0m\n",
       "\u001b[0;34m        Mel transform matrix\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    See also\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    librosa.util.normalize\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Notes\u001b[0m\n",
       "\u001b[0;34m    -----\u001b[0m\n",
       "\u001b[0;34m    This function caches at level 10.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    >>> melfb = librosa.filters.mel(22050, 2048)\u001b[0m\n",
       "\u001b[0;34m    >>> melfb\u001b[0m\n",
       "\u001b[0;34m    array([[ 0.   ,  0.016, ...,  0.   ,  0.   ],\u001b[0m\n",
       "\u001b[0;34m           [ 0.   ,  0.   , ...,  0.   ,  0.   ],\u001b[0m\n",
       "\u001b[0;34m           ...,\u001b[0m\n",
       "\u001b[0;34m           [ 0.   ,  0.   , ...,  0.   ,  0.   ],\u001b[0m\n",
       "\u001b[0;34m           [ 0.   ,  0.   , ...,  0.   ,  0.   ]])\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Clip the maximum frequency to 8KHz\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    >>> librosa.filters.mel(22050, 2048, fmax=8000)\u001b[0m\n",
       "\u001b[0;34m    array([[ 0.  ,  0.02, ...,  0.  ,  0.  ],\u001b[0m\n",
       "\u001b[0;34m           [ 0.  ,  0.  , ...,  0.  ,  0.  ],\u001b[0m\n",
       "\u001b[0;34m           ...,\u001b[0m\n",
       "\u001b[0;34m           [ 0.  ,  0.  , ...,  0.  ,  0.  ],\u001b[0m\n",
       "\u001b[0;34m           [ 0.  ,  0.  , ...,  0.  ,  0.  ]])\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    >>> import matplotlib.pyplot as plt\u001b[0m\n",
       "\u001b[0;34m    >>> fig, ax = plt.subplots()\u001b[0m\n",
       "\u001b[0;34m    >>> img = librosa.display.specshow(melfb, x_axis='linear', ax=ax)\u001b[0m\n",
       "\u001b[0;34m    >>> ax.set(ylabel='Mel filter', title='Mel filter bank')\u001b[0m\n",
       "\u001b[0;34m    >>> fig.colorbar(img, ax=ax)\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mfmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Initialize the weights\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_fft\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Center freqs of each FFT bin\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfftfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfft_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# 'Center freqs' of mel bands - uniformly spaced between limits\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmel_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mramps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfftfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# lower and upper slopes for all bins\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mramps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfdiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mramps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfdiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# .. then intersect them with each other and zero\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"slaney\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0menorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmel_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_mels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmel_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0menorm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Only check weights if f_mel[0] is positive\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# This means we have an empty channel somewhere\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Empty filters detected in mel frequency basis. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Some channels will produce empty responses. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Try increasing your sampling rate (and fmax) or \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"reducing n_mels.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.10/site-packages/librosa/filters.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??librosa_mel_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186edfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mel in module librosa.filters:\n",
      "\n",
      "mel(sr, n_fft, n_mels=128, fmin=0.0, fmax=None, htk=False, norm='slaney', dtype=<class 'numpy.float32'>)\n",
      "    Create a Mel filter-bank.\n",
      "    \n",
      "    This produces a linear transformation matrix to project\n",
      "    FFT bins onto Mel-frequency bins.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    sr        : number > 0 [scalar]\n",
      "        sampling rate of the incoming signal\n",
      "    \n",
      "    n_fft     : int > 0 [scalar]\n",
      "        number of FFT components\n",
      "    \n",
      "    n_mels    : int > 0 [scalar]\n",
      "        number of Mel bands to generate\n",
      "    \n",
      "    fmin      : float >= 0 [scalar]\n",
      "        lowest frequency (in Hz)\n",
      "    \n",
      "    fmax      : float >= 0 [scalar]\n",
      "        highest frequency (in Hz).\n",
      "        If `None`, use ``fmax = sr / 2.0``\n",
      "    \n",
      "    htk       : bool [scalar]\n",
      "        use HTK formula instead of Slaney\n",
      "    \n",
      "    norm : {None, 'slaney', or number} [scalar]\n",
      "        If 'slaney', divide the triangular mel weights by the width of the mel band\n",
      "        (area normalization).\n",
      "    \n",
      "        If numeric, use `librosa.util.normalize` to normalize each filter by to unit l_p norm.\n",
      "        See `librosa.util.normalize` for a full description of supported norm values\n",
      "        (including `+-np.inf`).\n",
      "    \n",
      "        Otherwise, leave all the triangles aiming for a peak value of 1.0\n",
      "    \n",
      "    dtype : np.dtype\n",
      "        The data type of the output basis.\n",
      "        By default, uses 32-bit (single-precision) floating point.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    M         : np.ndarray [shape=(n_mels, 1 + n_fft/2)]\n",
      "        Mel transform matrix\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    librosa.util.normalize\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This function caches at level 10.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> melfb = librosa.filters.mel(22050, 2048)\n",
      "    >>> melfb\n",
      "    array([[ 0.   ,  0.016, ...,  0.   ,  0.   ],\n",
      "           [ 0.   ,  0.   , ...,  0.   ,  0.   ],\n",
      "           ...,\n",
      "           [ 0.   ,  0.   , ...,  0.   ,  0.   ],\n",
      "           [ 0.   ,  0.   , ...,  0.   ,  0.   ]])\n",
      "    \n",
      "    \n",
      "    Clip the maximum frequency to 8KHz\n",
      "    \n",
      "    >>> librosa.filters.mel(22050, 2048, fmax=8000)\n",
      "    array([[ 0.  ,  0.02, ...,  0.  ,  0.  ],\n",
      "           [ 0.  ,  0.  , ...,  0.  ,  0.  ],\n",
      "           ...,\n",
      "           [ 0.  ,  0.  , ...,  0.  ,  0.  ],\n",
      "           [ 0.  ,  0.  , ...,  0.  ,  0.  ]])\n",
      "    \n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> fig, ax = plt.subplots()\n",
      "    >>> img = librosa.display.specshow(melfb, x_axis='linear', ax=ax)\n",
      "    >>> ax.set(ylabel='Mel filter', title='Mel filter bank')\n",
      "    >>> fig.colorbar(img, ax=ax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from librosa.filters import mel as librosa_mel_fn\n",
    "help(librosa_mel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be355db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "from librosa.util import pad_center\n",
    "librosa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c596953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pad_center in module librosa.util.utils:\n",
      "\n",
      "pad_center(data: 'np.ndarray', *, size: 'int', axis: 'int' = -1, **kwargs: 'Any') -> 'np.ndarray'\n",
      "    Pad an array to a target length along a target axis.\n",
      "    \n",
      "    This differs from `np.pad` by centering the data prior to padding,\n",
      "    analogous to `str.center`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> # Generate a vector\n",
      "    >>> data = np.ones(5)\n",
      "    >>> librosa.util.pad_center(data, size=10, mode='constant')\n",
      "    array([ 0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.])\n",
      "    \n",
      "    >>> # Pad a matrix along its first dimension\n",
      "    >>> data = np.ones((3, 5))\n",
      "    >>> librosa.util.pad_center(data, size=7, axis=0)\n",
      "    array([[ 0.,  0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.],\n",
      "           [ 1.,  1.,  1.,  1.,  1.],\n",
      "           [ 1.,  1.,  1.,  1.,  1.],\n",
      "           [ 1.,  1.,  1.,  1.,  1.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.]])\n",
      "    >>> # Or its second dimension\n",
      "    >>> librosa.util.pad_center(data, size=7, axis=1)\n",
      "    array([[ 0.,  1.,  1.,  1.,  1.,  1.,  0.],\n",
      "           [ 0.,  1.,  1.,  1.,  1.,  1.,  0.],\n",
      "           [ 0.,  1.,  1.,  1.,  1.,  1.,  0.]])\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : np.ndarray\n",
      "        Vector to be padded and centered\n",
      "    size : int >= len(data) [scalar]\n",
      "        Length to pad ``data``\n",
      "    axis : int\n",
      "        Axis along which to pad and center the data\n",
      "    **kwargs : additional keyword arguments\n",
      "        arguments passed to `np.pad`\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data_padded : np.ndarray\n",
      "        ``data`` centered and padded to length ``size`` along the\n",
      "        specified axis\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ParameterError\n",
      "        If ``size < data.shape[axis]``\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    numpy.pad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pad_center\n",
    "help(pad_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8243e245",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad_center() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpad_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: pad_center() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pad_center(np.ones(1000), 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0237628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa==0.8.1\n",
      "  Using cached librosa-0.8.1-py3-none-any.whl (203 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (1.22.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (1.2.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (23.0)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (0.55.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (0.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (1.10.1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (1.2.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from librosa==0.8.1) (5.1.1)\n",
      "Requirement already satisfied: setuptools in /home/ray/anaconda3/lib/python3.10/site-packages (from numba>=0.43.0->librosa==0.8.1) (67.4.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/ray/anaconda3/lib/python3.10/site-packages (from numba>=0.43.0->librosa==0.8.1) (0.38.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.8.1) (2.28.2)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.8.1) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ray/anaconda3/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa==0.8.1) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ray/anaconda3/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ray/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2022.12.7)\n",
      "Installing collected packages: librosa\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.10.0\n",
      "    Uninstalling librosa-0.10.0:\n",
      "      Successfully uninstalled librosa-0.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.11.1 requires cython==0.29.28, but you have cython 0.29.33 which is incompatible.\n",
      "tts 0.11.1 requires inflect==5.6.0, but you have inflect 6.0.2 which is incompatible.\n",
      "tts 0.11.1 requires librosa==0.8.0, but you have librosa 0.8.1 which is incompatible.\n",
      "tts 0.11.1 requires numpy==1.22.4; python_version == \"3.10\", but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed librosa-0.8.1\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c8a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ray\n",
    "parallelism_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005c4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 16:54:26,264\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.37.211:6379...\n",
      "2023-03-06 16:54:26,291\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_ha6my3r3kl4mn9jdylcqnfutq1/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-03-06 16:54:26,316\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_c450220a9fe1382fee31ab05c616a7cf.zip' (6.75MiB) to Ray cluster...\n",
      "2023-03-06 16:54:26,438\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_c450220a9fe1382fee31ab05c616a7cf.zip'.\n"
     ]
    }
   ],
   "source": [
    "lj_df = pd.read_csv(\n",
    "    \"https://uberduck-datasets-dirty.s3.us-west-2.amazonaws.com/lj_for_upload/metadata_formatted_100_edited.txt\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    quoting=3,\n",
    "    names=[\"path\", \"transcript\", \"speaker_id\"], # pitch path is implicit - this should be changed\n",
    ")\n",
    "speaker_ids = lj_df.speaker_id.tolist()\n",
    "speaker_ids_ds = ray.data.from_items(speaker_ids, parallelism=parallelism_length)\n",
    "speaker_ids_ds = speaker_ids_ds.map_batches(\n",
    "    lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c649575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>transcript</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>have now come into general use and are obvious...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>a little reduced in ugliness. The design of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>and the whole effect is a little too gray, owi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>It must be remembered, however, that most mode...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>s3://uberduck-datasets-dirty/lj_for_upload/lj_...</td>\n",
       "      <td>and these somewhat wiry letters are suitable f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  \\\n",
       "0   s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "1   s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "2   s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "3   s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "4   s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "..                                                ...   \n",
       "95  s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "96  s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "97  s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "98  s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "99  s3://uberduck-datasets-dirty/lj_for_upload/lj_...   \n",
       "\n",
       "                                           transcript  speaker_id  \n",
       "0   Printing, in the only sense with which we are ...           0  \n",
       "1                      in being comparatively modern.           0  \n",
       "2   For although the Chinese took impressions from...           0  \n",
       "3   produced the block books, which were the immed...           0  \n",
       "4   the invention of movable metal letters in the ...           0  \n",
       "..                                                ...         ...  \n",
       "95  have now come into general use and are obvious...           0  \n",
       "96  a little reduced in ugliness. The design of th...           0  \n",
       "97  and the whole effect is a little too gray, owi...           0  \n",
       "98  It must be remembered, however, that most mode...           0  \n",
       "99  and these somewhat wiry letters are suitable f...           0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d8681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = speaker_ids_ds.map_batches(\n",
    "    lambda table: table.rename(\n",
    "        columns={\n",
    "\n",
    "            \"value_1\": \"speaker_id\"\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caadd507",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'speaker_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_id\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'speaker_id'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +2s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(autoscaler +2s) Removing 2 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +7s) Removing 2 nodes of type worker-node-type-0 (idle).\n",
      "(autoscaler +12s) Removing 2 nodes of type worker-node-type-0 (idle).\n"
     ]
    }
   ],
   "source": [
    "output_dataset.speaker_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad241a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 16:50:22,749\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)->MapBatches(<lambda>)]\n",
      "MapBatches(<lambda>)->MapBatches(<lambda>): 100%|██████████| 100/100 [00:03<00:00, 28.88it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de3e774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     lj_df = pd.read_csv(\n",
    "#         \"https://uberduck-datasets-dirty.s3.us-west-2.amazonaws.com/vctk_mic1/all_with_embs.txt\",\n",
    "#         sep=\"|\",\n",
    "#         header=None,\n",
    "#         quoting=3,\n",
    "#         names=[\"path\", \"speaker_id\", \"transcript\", \"dataset_audio_file_id\", \"emb_path\"],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0c79b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lj_df['path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8722310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcripts_ds = ray.data.from_items(transcripts, parallelism=parallelism_length)\n",
    "# speaker_ids_ds = ray.data.from_items(speaker_ids, parallelism=parallelism_length)\n",
    "# dataset_audio_file_ids = ray.data.from_items(\n",
    "#     dataset_audio_files, parallelism=parallelism_length\n",
    "# )\n",
    "\n",
    "# pitch_paths_ds = ray.data.read_binary_files(\n",
    "#     pitch_paths,\n",
    "#     parallelism=parallelism_length,\n",
    "#     meta_provider=FastFileMetadataProvider(),\n",
    "#     ray_remote_args={\"num_cpus\": 0.2},\n",
    "# )\n",
    "\n",
    "# audio_ds = audio_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "# transcripts_ds = transcripts_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "# dataset_audio_file_ids = dataset_audio_file_ids.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "# paths_ds = paths.map_batches(lambda x: x, batch_format=\"pyarrow\", batch_size=None)\n",
    "# pitches_paths_ds = pitch_paths_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n",
    "# speaker_ids_ds = speaker_ids_ds.map_batches(\n",
    "#     lambda x: x, batch_format=\"pyarrow\", batch_size=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a5892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
